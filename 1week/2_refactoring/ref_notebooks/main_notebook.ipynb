{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import *\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DATA \n",
    "DATA_PATHS = [\"../0_datas/rt-polarity.neg\", \"../0_datas/rt-polarity.pos\"]\n",
    "\n",
    "NEGATIVE_DATAS = read_data(DATA_PATHS[0], 1)\n",
    "POSITVIE_DATAS = read_data(DATA_PATHS[1], 0)\n",
    "total_data = NEGATIVE_DATAS + POSITVIE_DATAS\n",
    "\n",
    "train_data, test_data = train_test_split(total_data, 0.9)\n",
    "train_data, val_data = train_test_split(train_data, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocabs = Vocabs()\n",
    "text_vocabs.build_vocabs(train_data[:,0])\n",
    "\n",
    "train_x_values = text_vocabs.stoi(train_data[:,0].tolist())\n",
    "train_y_values = train_data[:,1]\n",
    "train_data = np.array([*zip(train_x_values, train_y_values)])\n",
    "\n",
    "val_x_values = text_vocabs.stoi(val_data[:,0].tolist())\n",
    "val_y_values = val_data[:,1]\n",
    "val_data = np.array([*zip(val_x_values, val_y_values)])\n",
    "\n",
    "test_x_values = text_vocabs.stoi(test_data[:,0].tolist())\n",
    "test_y_values = test_data[:,1]\n",
    "test_data = np.array([*zip(test_x_values, test_y_values)])\n",
    "\n",
    "train_dataset = DiverseDataset(train_data)\n",
    "valid_dataset = DiverseDataset(val_data)\n",
    "test_dataset = DiverseDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import *\n",
    "from src.model import CNN1d, make_init_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(text_vocabs)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
<<<<<<< HEAD
    "PAD_IDX = 0\n",
=======
    "PAD_IDX = 1\n",
>>>>>>> 5a7fda032d3ffd46d74089c5328441aa4226ae64
    "cnn_model = CNN1d(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/torchstudy/1week/2_refactoring/src/model.py:15: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  my_data_vectors.append(torch.FloatTensor(w2v_model[token]))\n"
     ]
    }
   ],
   "source": [
    "w2v_path = '../0_datas/GoogleNews-vectors-negative300.bin.gz'\n",
    "vectors_in_trainset = make_init_vectors(w2v_path, text_vocabs.vocab_dict.keys(), wv_dim = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0032, -0.0123,  0.0102,  ..., -0.0059, -0.0010, -0.0112],\n",
       "        [ 0.0098,  0.0116,  0.0092,  ...,  0.0035,  0.0131, -0.0031],\n",
       "        [-0.0116,  0.0067, -0.0118,  ...,  0.0099, -0.0075, -0.0092],\n",
       "        ...,\n",
       "        [-0.0923,  0.1094,  0.0635,  ...,  0.0283,  0.0884,  0.1904],\n",
       "        [-0.3887,  0.0801,  0.1875,  ..., -0.6250,  0.1719, -0.1953],\n",
       "        [-0.1079,  0.4531,  0.1699,  ..., -0.0106,  0.1660,  0.1875]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.embedding.weight.data.copy_(vectors_in_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adadelta(cnn_model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "device = 'cpu'\n",
    "cnn_model = cnn_model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 | Valid Loss: 0.667 |  Valid Acc: 68.92%\n",
      "Epoch : 1 | Valid Loss: 0.612 |  Valid Acc: 74.30%\n",
      "Epoch : 2 | Valid Loss: 0.541 |  Valid Acc: 74.97%\n",
      "Epoch : 3 | Valid Loss: 0.492 |  Valid Acc: 77.30%\n",
      "Epoch : 4 | Valid Loss: 0.559 |  Valid Acc: 71.56%\n",
      "Epoch : 5 | Valid Loss: 0.455 |  Valid Acc: 78.81%\n",
      "Epoch : 6 | Valid Loss: 0.446 |  Valid Acc: 79.06%\n",
      "Epoch : 7 | Valid Loss: 0.516 |  Valid Acc: 74.84%\n",
      "Epoch : 8 | Valid Loss: 0.424 |  Valid Acc: 80.05%\n",
      "Epoch : 9 | Valid Loss: 0.394 |  Valid Acc: 82.09%\n",
      "TEST LOSS : 0.4641309281190236 | TEST ACC: 0.7871713836987814\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_FOLDS = 10\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "train_iterater = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_padd)\n",
    "valid_iterater = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_padd)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_padd)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc = train(cnn_model, train_iterater, optimizer, criterion, apply_max_norm=True, device=device)\n",
    "    valid_loss, valid_acc = evaluate(cnn_model, valid_iterater, criterion, device=device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(cnn_model.state_dict(), 'tut4-model.pt')\n",
    "\n",
    "    print(f'Epoch : {epoch} | Valid Loss: {valid_loss:.3f} |  Valid Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "test_loss, test_acc = evaluate(cnn_model, test_iterator, criterion, device=device)\n",
    "print(f\"TEST LOSS : {test_loss} | TEST ACC: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold Average Loss: 0.667 |  Average Acc: 80.63%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "from src.train import *\n",
    "from src.model import CNN1d, make_init_vectors\n",
    "\n",
    "N_FOLDS = 10\n",
    "N_EPOCHS = 10\n",
    "\n",
    "DATA_PATHS = [\"../0_datas/rt-polarity.neg\", \"../0_datas/rt-polarity.pos\"]\n",
    "\n",
    "NEGATIVE_DATAS = read_data(DATA_PATHS[0], 1)\n",
    "POSITVIE_DATAS = read_data(DATA_PATHS[1], 0)\n",
    "total_data = NEGATIVE_DATAS + POSITVIE_DATAS\n",
    "\n",
    "train_data_kfold, _ = train_test_split(total_data, 1.0)\n",
    "\n",
    "text_vocabs = Vocabs()\n",
    "text_vocabs.build_vocabs(train_data_kfold[:,0])\n",
    "\n",
    "train_x_values = text_vocabs.stoi(train_data_kfold[:,0].tolist())\n",
    "train_y_values = train_data_kfold[:,1]\n",
    "train_data_kfold = np.array([*zip(train_x_values, train_y_values)])\n",
    "\n",
    "train_dataset = DiverseDataset(train_data_kfold)\n",
    "\n",
    "kf = KFold(n_splits=N_FOLDS, random_state=12, shuffle=True)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "fold = 1\n",
    "kfold_acc = []\n",
    "kfold_error = []\n",
    "\n",
    "for train_index, test_index in kf.split(np.arange(len(train_dataset))):\n",
    "\n",
    "    device = 'cuda:0'\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "    \n",
    "    train_fold_subset = torch.utils.data.Subset(train_dataset, train_index)\n",
    "    train_fold_iterator = torch.utils.data.DataLoader(train_fold_subset, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn_padd)\n",
    "\n",
    "    test_fold_subset = torch.utils.data.Subset(train_dataset, test_index)\n",
    "    test_fold_iterator = torch.utils.data.DataLoader(test_fold_subset, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn_padd)\n",
    "\n",
    "    INPUT_DIM = len(text_vocabs)\n",
    "    EMBEDDING_DIM = 300\n",
    "    N_FILTERS = 100\n",
    "    FILTER_SIZES = [3,4,5]\n",
    "    OUTPUT_DIM = 1\n",
    "    DROPOUT = 0.5\n",
<<<<<<< HEAD
    "    PAD_IDX = 0\n",
=======
    "    PAD_IDX = 1\n",
>>>>>>> 5a7fda032d3ffd46d74089c5328441aa4226ae64
    "    \n",
    "    cnn_model = CNN1d(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "    cnn_model = cnn_model.to(device)\n",
    "    w2v_path = '../0_datas/GoogleNews-vectors-negative300.bin.gz'\n",
    "    vectors_in_trainset = make_init_vectors(w2v_path, text_vocabs.vocab_dict.keys(), wv_dim = EMBEDDING_DIM)\n",
    "    cnn_model.embedding.weight.data.copy_(vectors_in_trainset)\n",
    "    \n",
    "    optimizer = optim.Adadelta(cnn_model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        train_loss, train_acc = train(cnn_model, train_fold_iterator, optimizer, criterion, apply_max_norm=True, device=device)\n",
    "\n",
    "    valid_loss, valid_acc = evaluate(cnn_model, test_fold_iterator, criterion, device=device)\n",
    "    kfold_acc.append(valid_acc)\n",
    "    kfold_error.append(valid_loss)\n",
    "\n",
    "    fold += 1\n",
    "print(f'K-fold Average Loss: {np.mean(kfold_error):.3f} |  Average Acc: {np.mean(kfold_acc)*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def model_with_dropout_scailing(model, dropout_rate=0.5):\n",
    "    inf_model = copy.deepcopy(model)\n",
    "    for name, param in inf_model.named_parameters():\n",
    "        if 'bias' not in name:\n",
    "            param = param * dropout_rate\n",
    "    return inf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_model = model_with_dropout_scailing(cnn_model, dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0044]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_text = [\"it is good good <PAD> <PAD>\"]\n",
    "input_tensor = torch.Tensor(text_vocabs.stoi(input_text)).to(torch.int64).to(device)\n",
    "with torch.no_grad(): \n",
    "    print(torch.sigmoid(inf_model(input_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
