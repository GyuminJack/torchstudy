{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_str(string, TREC=False):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip() if TREC else string.strip().lower()\n",
    "\n",
    "def read_data(path, label):\n",
    "    ret = []\n",
    "    with open(path, \"r\", encoding = \"ISO-8859-1\") as f:\n",
    "        for line in f.readlines():\n",
    "            ret.append([clean_str(line.replace(\"\\n\",\"\")),label])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_ratio = 0.9):\n",
    "    import random\n",
    "    import math\n",
    "    import numpy\n",
    "    _len = len(data)\n",
    "    random.shuffle(data)\n",
    "    train_data = np.array(data[:math.ceil(_len*train_ratio)])\n",
    "    test_data = np.array(data[math.ceil(_len*train_ratio):])\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATHS = [\"./datas/CNN_sentence/rt-polarity.neg\", \"./datas/CNN_sentence/rt-polarity.pos\"]\n",
    "NEGATIVE_DATAS = read_data(DATA_PATHS[0], 1)\n",
    "POSITVIE_DATAS = read_data(DATA_PATHS[1], 0)\n",
    "total_data = NEGATIVE_DATAS + POSITVIE_DATAS\n",
    "\n",
    "train_data, test_data = train_test_split(total_data, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기능\n",
    "\n",
    "#### vocabs\n",
    "- vocab_dict\n",
    "- itos\n",
    "- stoi\n",
    "\n",
    "#### dataset\n",
    "- parameter\n",
    "    - MAX_SEQ_LEN -> for padding\n",
    "    - \n",
    "- in\n",
    "    - dataset\n",
    "- out\n",
    "    - word_index, label\n",
    "\n",
    "#### dataloader\n",
    "- in\n",
    "    - dataset\n",
    "- out\n",
    "    - batch input, batch label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabs:\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_dict)\n",
    "    def build_vocabs(self, sentence_list):\n",
    "        from collections import defaultdict\n",
    "        self.vocab_dict = defaultdict(lambda: '<UNK>')\n",
    "        self.vocab_dict[\"<UNK>\"] = 0\n",
    "        self.vocab_dict[\"<PAD>\"] = 1\n",
    "        \n",
    "        _index = 2\n",
    "        for sentence in sentence_list:\n",
    "            tokens_list = sentence.split(\" \")\n",
    "            for word in tokens_list:\n",
    "                if word in self.vocab_dict:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.vocab_dict[word] = _index\n",
    "                    _index += 1\n",
    "        self.index_dict = {v:k for k, v in self.vocab_dict.items()}\n",
    "    def stoi(self, sentence):\n",
    "        if type(sentence) == str:\n",
    "            return [self.vocab_dict[word] for word in sentence.split(\" \")]\n",
    "        elif type(sentence) == list:\n",
    "            return [self.stoi(i) for i in sentence]\n",
    "\n",
    "    def itos(self, indices):\n",
    "        if type(indices[0]) == int :\n",
    "            return \" \".join([self.index_dict[index] for index in indices if self.index_dict[index] != '<PAD>'])\n",
    "        elif type(indices) == list:\n",
    "            return [self.itos(i) for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocabs = Vocabs()\n",
    "text_vocabs.build_vocabs(train_data[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it is risky , intelligent , romantic and rapturous from start to finish',\n",
       " 'great story , bad idea for a movie']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-8825a540519f>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  test_data = np.array([*zip(test_x_values, test_y_values)])\n"
     ]
    }
   ],
   "source": [
    "test_x_values = text_vocabs.stoi(train_data[:10,0].tolist())\n",
    "test_y_values = train_data[:10,1]\n",
    "test_data = np.array([*zip(test_x_values, test_y_values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['it', 'is', 'very', 'good'], ['i', 'am', 'not']]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vocabs.itos([[2, 3, 399, 642], [700, 10087, 155]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, np_data, max_seq_len):\n",
    "        self.x_data = np_data[:,0]\n",
    "        self.y_data = np_data[:,1].reshape(-1,1).astype(int)\n",
    "        self.max_len = max_seq_len\n",
    "        self.pad_num = 1\n",
    "        super()\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data_cut_pad(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y\n",
    "    def data_cut_pad(self, data):\n",
    "        if len(data) >= self.max_len:\n",
    "            data = data[:self.max_len]\n",
    "        elif len(data) < self.max_len:\n",
    "            data = data + [self.pad_num] * (self.max_len- len(data))\n",
    "        return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_data, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  2,  58,  75,   5,  76,  77,  65,   2,  58,  78,  12,  60,  19,  79,\n",
      "          80,  81,  82,  21,  83,   1],\n",
      "        [ 21,  38,   3,  39,  40,  41,  42,  43,  21,  44,  45,  12,  46,  47,\n",
      "          21,  48,   3,  49,  21,  50],\n",
      "        [104, 105, 106, 107, 108, 109, 110,   8, 111, 112,  19, 113,   5, 114,\n",
      "          19, 115, 116, 117,  82,  21],\n",
      "        [  2,  58,  84,  85,   8,  85,  12,  86,  21,  87,  42,  88,   3,  89,\n",
      "          90,  49,  91,   5,  92,   2]]), tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])]\n",
      "[tensor([[ 2, 58, 59, 12, 60, 21, 61, 42, 62, 63, 19, 64,  5, 65, 66, 58, 67, 42,\n",
      "         68, 69],\n",
      "        [ 2,  3,  4,  5,  6,  5,  7,  8,  9, 10, 11, 12, 13,  1,  1,  1,  1,  1,\n",
      "          1,  1],\n",
      "        [14, 15,  5, 16, 17, 18, 19, 20,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1],\n",
      "        [21, 20,  3, 22, 23, 24,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "          1,  1]]), tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])]\n",
      "[tensor([[ 25,  19,  26,   5,  27,  21,  28,   7,  29,   5,  25,  19,  30,  31,\n",
      "          32,  21,  33,  34,  35,   8],\n",
      "        [ 19,  94,   5,  95,   5,  96,  97,  98,  34,  19,  99, 100, 101,  58,\n",
      "          21, 102,  34, 103,   1,   1]]), tensor([[1.],\n",
      "        [0.]])]\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(sample_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[0], embedding_dim))\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[1], embedding_dim))\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[2], embedding_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(text_vocabs)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = 0\n",
    "cnn_model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 21,  38,   3,  39,  40,  41,  42,  43,  21,  44,  45,  12,  46,  47,\n",
      "          21,  48,   3,  49,  21,  50],\n",
      "        [ 19,  94,   5,  95,   5,  96,  97,  98,  34,  19,  99, 100, 101,  58,\n",
      "          21, 102,  34, 103,   1,   1],\n",
      "        [ 14,  15,   5,  16,  17,  18,  19,  20,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1],\n",
      "        [  2,   3,   4,   5,   6,   5,   7,   8,   9,  10,  11,  12,  13,   1,\n",
      "           1,   1,   1,   1,   1,   1]])\n",
      "tensor([[104, 105, 106, 107, 108, 109, 110,   8, 111, 112,  19, 113,   5, 114,\n",
      "          19, 115, 116, 117,  82,  21],\n",
      "        [  2,  58,  84,  85,   8,  85,  12,  86,  21,  87,  42,  88,   3,  89,\n",
      "          90,  49,  91,   5,  92,   2],\n",
      "        [ 25,  19,  26,   5,  27,  21,  28,   7,  29,   5,  25,  19,  30,  31,\n",
      "          32,  21,  33,  34,  35,   8],\n",
      "        [ 21,  20,   3,  22,  23,  24,   2,   1,   1,   1,   1,   1,   1,   1,\n",
      "           1,   1,   1,   1,   1,   1]])\n",
      "tensor([[ 2, 58, 59, 12, 60, 21, 61, 42, 62, 63, 19, 64,  5, 65, 66, 58, 67, 42,\n",
      "         68, 69],\n",
      "        [ 2, 58, 75,  5, 76, 77, 65,  2, 58, 78, 12, 60, 19, 79, 80, 81, 82, 21,\n",
      "         83,  1]])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(sample_batched[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7783],\n",
      "        [-0.0789],\n",
      "        [-0.1006],\n",
      "        [-0.3747]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.0443],\n",
      "        [ 0.2030],\n",
      "        [-0.0858],\n",
      "        [ 0.3317]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.1305],\n",
      "        [-0.2868]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    train_x = sample_batched[0]\n",
    "    train_y = sample_batched[1]\n",
    "    print(cnn_model(sample_batched[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
