{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timeit\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3가지의 항목을 포함하고 있습니다.\n",
    "\n",
    "### 1. Copare Dataloaders : 패딩을 하는 데이터 로더 비교\n",
    "### 2. L2-norm weight scailing (include Gradient clipping)\n",
    "### 3. W2V OOV (out of vocabulary) 처리\n",
    "### 4. K-fold 처리\n",
    "### 5. 테스트 시에 dropout rate scailing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compare Dataloaders\n",
    "\n",
    "##### 3가지의 데이터 로더를 비교합니다.\n",
    "1. dataset 빌드시에 데이터를 고정길이로 패딩 후 모델에 입력\n",
    "2. dataloader에서 가져올때 고정길이로 패딩 후 모델에 입력\n",
    "3. dataloader에 rnn_padsequence 메서드를 통해 가변길이(배치중 가장 긴 길이)로 패딩 후 모델에 입력 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (def) Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_str(string, TREC=False):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip() if TREC else string.strip().lower()\n",
    "\n",
    "def read_data(path, label):\n",
    "    ret = []\n",
    "    with open(path, \"r\", encoding = \"ISO-8859-1\") as f:\n",
    "        for line in f.readlines():\n",
    "            ret.append([clean_str(line.replace(\"\\n\",\"\")),label])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, train_ratio = 0.9):\n",
    "    import random\n",
    "    import math\n",
    "    import numpy\n",
    "    _len = len(data)\n",
    "    random.shuffle(data)\n",
    "    train_data = np.array(data[:math.ceil(_len*train_ratio)])\n",
    "    test_data = np.array(data[math.ceil(_len*train_ratio):])\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Class) Make Vocabs to make Index easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabs:\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_dict)\n",
    "    \n",
    "    def set_tokenizer(self, tokenizer=None):\n",
    "        if tokenizer == None:\n",
    "            tokenizer = lambda x: x.split(\" \")\n",
    "        else:\n",
    "            tokenizer = tokenizer\n",
    "            \n",
    "    def build_vocabs(self, sentence_list):\n",
    "        from collections import defaultdict\n",
    "        self.vocab_dict = defaultdict(lambda: 0)\n",
    "        self.vocab_dict[\"<UNK>\"] = 0\n",
    "        self.vocab_dict[\"<PAD>\"] = 1\n",
    "        \n",
    "        _index = 2\n",
    "        for sentence in sentence_list:\n",
    "            tokens_list = sentence.split(\" \")\n",
    "            for word in tokens_list:\n",
    "                if word in self.vocab_dict:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.vocab_dict[word] = _index\n",
    "                    _index += 1\n",
    "        self.index_dict = {v:k for k, v in self.vocab_dict.items()}\n",
    "        \n",
    "    def stoi(self, sentence):\n",
    "        if type(sentence) == str:\n",
    "            return [self.vocab_dict[word] for word in self.tokenizer(sentence)]\n",
    "        elif type(sentence) == list:\n",
    "            return [self.stoi(i) for i in sentence]\n",
    "\n",
    "    def itos(self, indices):\n",
    "        if type(indices[0]) == int :\n",
    "            return \" \".join([self.index_dict[index] for index in indices if self.index_dict[index] != '<PAD>'])\n",
    "        elif type(indices) == list:\n",
    "            return [self.itos(i) for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Class) Sample model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[0], embedding_dim))\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[1], embedding_dim))\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[2], embedding_dim))\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Class) Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pad in dataset (static)\n",
    "class DiverseDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, np_data):\n",
    "        self.x_data = np_data[:,0]\n",
    "        self.y_data = np_data[:,1].reshape(-1,1).astype(int)\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.IntTensor(self.x_data[idx]).to(torch.int64)\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pad in dataset (static)\n",
    "class RuntimeDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, np_data, max_seq_len):\n",
    "        self.x_data = np_data[:,0]\n",
    "        self.y_data = np_data[:,1].reshape(-1,1).astype(int)\n",
    "        self.max_len = max_seq_len\n",
    "        self.pad_num = 1\n",
    "        super()\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.IntTensor(self.data_cut_pad(self.x_data[idx])).to(torch.int64)\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y\n",
    "    def data_cut_pad(self, data):\n",
    "        if len(data) >= self.max_len:\n",
    "            data = data[:self.max_len]\n",
    "        elif len(data) < self.max_len:\n",
    "            data = data + [self.pad_num] * (self.max_len- len(data))\n",
    "        return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildtimeDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, np_data, max_seq_len):\n",
    "\n",
    "        self.max_len = max_seq_len\n",
    "        self.pad_num = 1\n",
    "        self.x_data = self.padding(np_data[:,0])\n",
    "        self.y_data = np_data[:,1].reshape(-1,1).astype(int)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.IntTensor(self.x_data[idx]).to(torch.int64)\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y\n",
    "    \n",
    "    def padding(self, bdata):\n",
    "        adata = []\n",
    "        for data in bdata:\n",
    "            if len(data) >= self.max_len:\n",
    "                data = data[:self.max_len]\n",
    "            elif len(data) < self.max_len:\n",
    "                data = data + [self.pad_num] * (self.max_len- len(data))\n",
    "            adata.append(data)\n",
    "        return np.array(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (RUN) Reading DATAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read DATA \n",
    "DATA_PATHS = [\"./datas/rt-polarity.neg\", \"./datas/rt-polarity.pos\"]\n",
    "NEGATIVE_DATAS = read_data(DATA_PATHS[0], 1)\n",
    "POSITVIE_DATAS = read_data(DATA_PATHS[1], 0)\n",
    "total_data = NEGATIVE_DATAS + POSITVIE_DATAS\n",
    "train_data, test_data = train_test_split(total_data, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (RUN) build Vocab & Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocabs = Vocabs()\n",
    "text_vocabs.build_vocabs(train_data[:,0])\n",
    "\n",
    "INPUT_DIM = len(text_vocabs)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
<<<<<<< HEAD
    "PAD_IDX = 0\n",
=======
    "PAD_IDX = 1\n",
>>>>>>> 5a7fda032d3ffd46d74089c5328441aa4226ae64
    "cnn_model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (RUN) String to Index & make numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_values = text_vocabs.stoi(train_data[:,0].tolist())\n",
    "train_y_values = train_data[:,1]\n",
    "train_data = np.array([*zip(train_x_values, train_y_values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data의 최대 길이 : 56\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data의 최대 길이 : {np.max([len(i) for i in train_data[:,0]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (def) Collate fn ( Using torch.nn.utils.rnn.pad_sequence )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad in collate_fn\n",
    "def collate_fn_padd(batch):\n",
    "    ## get sequence lengths\n",
    "    device = \"cpu\"\n",
    "    lengths = torch.tensor([t[0].shape[0] for t in batch], dtype=torch.int).to(device)\n",
    "    ## padd\n",
    "    batch = [t[0] for t in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, padding_value=1).to(torch.int64)\n",
    "    return batch, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (RUN) Compare three Dataset & Dataloader\n",
    "- make dataset\n",
    "- make dataloader\n",
    "- make output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.5 s ± 430 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "21.1 s ± 248 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.02 s ± 6.08 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "21.2 s ± 153 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "21 s ± 58.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.02 s ± 9.51 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "21.2 s ± 230 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "21.4 s ± 349 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.03 s ± 3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "21.7 s ± 980 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "22.1 s ± 1.46 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.24 s ± 129 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "func_time = []\n",
    "BATCH_SIZE = 256\n",
    "for seq_len in range(10, 50, 10):\n",
    "    runtime =  %timeit -o [cnn_model(a[0]) for a in DataLoader(RuntimeDataset(train_data, i), batch_size=BATCH_SIZE, shuffle=True)]\n",
    "    buildtime =  %timeit -o [cnn_model(a[0]) for a in DataLoader(BuildtimeDataset(train_data, i), batch_size=BATCH_SIZE, shuffle=True)]\n",
    "    collatetime = %timeit -o [cnn_model(a[0]) for a in DataLoader(DiverseDataset(train_data), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_padd)]\n",
    "    func_time.append([seq_len, runtime, buildtime, collatetime])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, index1, index2 = [], [], []\n",
    "for item in func_time:\n",
    "    seq_time = item[0]\n",
    "    result_dict[seq_time] = dict()\n",
    "    times = item[1:]\n",
    "    for i, name in enumerate([\"runtime\",\"buildtime\",\"collatetime\"]):\n",
    "        for v in times[i].all_runs:\n",
    "            index1.append(str(seq_time))\n",
    "            index2.append(name)\n",
    "            value.append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (RUN) plot by dataloader type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='seq_len', ylabel='value'>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "df1 = pd.DataFrame({'value':value, 'seq_len':index1, 'dl_type':index2})\n",
    "df1 = df1.set_index(['seq_len','dl_type'])\n",
    "sns.lineplot(data=df1, x=\"seq_len\", y=\"value\", hue=\"dl_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. L2 regulize layer\n",
    "\n",
    "### 2.1 Max Norm Constraint\n",
    "If a hidden unit's weight vector's L2 norm L ever gets bigger than a certain max value c, multiply the weight vector by c/L. \n",
    "(만약, 히든유닛들의 가중치 벡터의 L2 norm(L)의 값이 특정 최대값(c)보다 크다면, 가중치 벡터에 c/L값을 곱해버린다)\n",
    "\n",
    "Enforce it immediately after each weight vector update or after every X gradient update.\n",
    "(이것은 가중치 벡터가 업데이터 된이후나, 모든 X의 그래디언트가 업데이트 된 직후에 실행한다.)\n",
    "\n",
    "This constraint is another form of regularization. While L2 penalizes high weights using the loss function, \"max norm\" acts directly on the weights. \n",
    "(이것은 regulizer의 한 종류이며, L2는 loss함수에서 높은 가중치만을 사용하지만, max-norm의 경우 가중치에 직접 동작한다.)\n",
    "\n",
    "\n",
    "L2 exerts a constant pressure to move the weights near zero which could throw away useful information when the loss function doesn't provide incentive for the weights to remain far from zero.\n",
    "(L2 제약은 가중치가 0 근처로 가게되어 유용한 정보를 주게되는데, loss 함수는 가중치에 incentive주지 못하게 하면서 0에서 떨어지지 못하게 한다)\n",
    "\n",
    "On the other hand, \"max norm\" never drives the weights to near zero. As long as the norm is less than the constraint value, the constraint has no effect.\n",
    "(max-norm의 경우에는 절대로 0근처로 보내지 못한. 가중치의 Norm 값이 특정 c 값보다 작다면 절대 영향을 미칠 수 없다)\n",
    "\n",
    "Reference : https://github.com/kevinzakka/pytorch-goodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_norm_scailing(model, max_val=3, eps=1e-8):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:\n",
    "            norm = param.norm(2, dim=0, keepdim=True)\n",
    "            \n",
    "            # torch.cla\n",
    "            desired = torch.clamp(norm, 0, max_val)\n",
    "            param = param * (desired / (eps + norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradient Clipping\n",
    "\n",
    "- Gradient 방향은 유지, 값의 크기를 깎아냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "max_norm = 5\n",
    "optimizier = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 L2 regularization\n",
    "- loss에 weigth의 L2값을 다 더해버림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam has l2 regularization option\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Another Method\n",
    "l2_lambda = 0.01\n",
    "l2_reg = torch.tensor(0.)\n",
    "for param in model.parameters():\n",
    "    l2_reg += torch.norm(param)\n",
    "loss += l2_lambda * l2_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. OOV initialize\n",
    "\n",
    "- Training 데이터의 경우 Task Specific 한데, 만일 global distribution이라고 추측한 W2V 모델에 traning data의 단어가 없으면 어떻게 할까\n",
    "  - 일단 논문에서는 기존 w2v의 variance를 구하고 이를 uniform distribution의 [-a, a]구간으로 샘플링\n",
    "- 조금 다른 이야기지만 Subword encoding이나 Byte-pair 인코딩을 통해서 OOV가 나지 않도록 최대한 줄여볼 수 있음\n",
    "  - Bert? 의 경우는 Sentence Piece 알고리즘을 통해 Vocab을 추출하고 3만5천개 정도로도 학습했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vectors(w2v_moodel, my_vocabs, wv_dim = 300):\n",
    "    lower = -(np.std(w2v_model.vectors, axis=0)**2)\n",
    "    upper = lower*-1\n",
    "    my_data_vectors = []\n",
    "    w2v_tokens = w2v_model.vocab.keys()\n",
    "    for token in notebook.tqdm(my_vocabs):\n",
    "        if token in w2v_tokens:\n",
    "            my_data_vectors.append(torch.FloatTensor(w2v_model[token]))\n",
    "        else:\n",
    "            my_data_vectors.append(torch.FloatTensor(np.random.uniform(lower, upper)))\n",
    "    return my_data_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format('./datas/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a5e7ec95914a07bfe865cf1573a0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "my_vocabs = {\"UNK\" : 0, \"PAD\" : 1, \"good\" : 2}\n",
    "vectors_in_trainset = init_vectors(w2v_model, my_vocabs.keys(), wv_dim = 300)\n",
    "\n",
    "# model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.K-fold data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train data를 10개의 fold로 분리해 분리된 것을 돌아가며 validataion 셋으로 구성\n",
    "- Subset 메서드를 통해 dataset에서 특정 인덱스만 골라내고, 이를 dataloader로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo code\n",
    "kf = KFold(n_splits=10, random_state=12, shuffle=True)\n",
    "val_loss = [] \n",
    "for train_index, test_index in kf.split(np.arange(100)):\n",
    "    train_fold_subset = torch.utils.data.Subset(train_dataset, train_index)\n",
    "    train_fold_loader = torch.utils.data.DataLoader(train_subset, batch_size=64)\n",
    "\n",
    "    val_fold_subset = torch.utils.data.Subset(train_dataset, test_index)\n",
    "    val_fold_loader = torch.utils.data.DataLoader(train_subset, batch_size=64)\n",
    "    \n",
    "    train(train_fold_loader)\n",
    "    loss = val(val_fold_loader)\n",
    "    val_loss.append(val_loss)\n",
    "print(np.mean(val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. dropout rate scailing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_scailing(model, dropout_rate=0.5, eps=1e-8):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:\n",
    "            param = param * dropout_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
